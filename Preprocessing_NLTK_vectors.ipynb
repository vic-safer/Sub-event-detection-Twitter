{"cells":[{"cell_type":"markdown","metadata":{"id":"Q4N5ACOO3bBQ"},"source":["# Preprocessing of the training data to get NLTK vectors"]},{"cell_type":"markdown","source":["### Definition of the paths"],"metadata":{"id":"ehAg7Cbo4ac7"}},{"cell_type":"code","source":["path_training_dest = \"C:/Users/victo/OneDrive/Desktop/INF554/Projet Twitter/df_train_NLTK.csv\"\n","train_path = \"C:/Users/victo/OneDrive/Desktop/INF554/Projet Twitter/challenge_data/train_tweets\"\n","\n","path_test_dest = \"C:/Users/victo/OneDrive/Desktop/INF554/Projet Twitter/df_test_NLTK.csv\"\n","test_path = \"C:/Users/victo/OneDrive/Desktop/INF554/Projet Twitter/challenge_data/eval_tweets\""],"metadata":{"id":"5st9XvT-339Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Import of useful libraries and modules"],"metadata":{"id":"Il4bR72X4gxF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wIoxCjEV3bBW"},"outputs":[],"source":["import pandas as pd\n","import os\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","import seaborn as sb\n","import openpyxl\n","import plotly.express as px\n","import plotly.graph_objects as go\n","import torch\n","import matplotlib.pyplot as plt\n","from gensim.models import Word2Vec\n","from gensim.utils import simple_preprocess\n","import os\n","import re\n","import gensim.downloader as api\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.dummy import DummyClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DG93YDbp3bBZ","outputId":"0223dbe9-fa40-4835-f236-095a17b06456"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\victo\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\victo\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["# Download some NLP models for processing, optional\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","# Load GloVe model with Gensim's API\n","embeddings_model = api.load(\"glove-twitter-200\")  # 200-dimensional GloVe embeddings"]},{"cell_type":"markdown","source":["Functions from the teachers' code"],"metadata":{"id":"d9U93ITx3lJv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gtw1W0BA3bBa"},"outputs":[],"source":["# Function to compute the average word vector for a tweet\n","def get_avg_embedding(tweet, model, vector_size=200):\n","    words = tweet.split()  # Tokenize by whitespace\n","    word_vectors = [model[word] for word in words if word in model]\n","    if not word_vectors:  # If no words in the tweet are in the vocabulary, return a zero vector\n","        return np.zeros(vector_size)\n","    return np.mean(word_vectors, axis=0)\n","\n","def preprocess_text(text):\n","    # Lowercasing\n","    text = text.lower()\n","    # Remove punctuation\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    # Remove numbers\n","    text = re.sub(r'\\d+', '', text)\n","    # Tokenization\n","    words = text.split()\n","    # Remove stopwords\n","    stop_words = set(stopwords.words('english'))\n","    words = [word for word in words if word not in stop_words]\n","    # Lemmatization\n","    lemmatizer = WordNetLemmatizer()\n","    words = [lemmatizer.lemmatize(word) for word in words]\n","    return ' '.join(words)"]},{"cell_type":"markdown","source":["### Preprocessing of the training data"],"metadata":{"id":"iGzteA-M4lRF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kjGfI8L83bBb"},"outputs":[],"source":["dataframes = []\n","\n","for fichier in os.listdir(train_path):\n","    if fichier.endswith('.csv'):\n","        chemin_complet = os.path.join(train_path, fichier)\n","        df = pd.read_csv(chemin_complet)\n","        labels = df.groupby('PeriodID')[\"EventType\"].sum().reset_index()\n","        labels[\"EventType\"] = labels[\"EventType\"].apply(lambda x: 1 if x>0 else 0)\n","        period_max = np.max(df[\"PeriodID\"].tolist())\n","        df_by_period = pd.DataFrame()\n","        df_by_period[\"PeriodID\"] = np.arange(0,period_max+1,1)\n","\n","        #pour le taux d'accroissement, on ne considère que les tweets EFFECTIVEMENT publiés pdt la période\n","        tweets_per_period = df.groupby('PeriodID').size().reset_index(name='TweetCount')\n","        tweets_per_period['GrowthRate'] = tweets_per_period['TweetCount'].pct_change() * 100\n","\n","        #pour le nb de tweets par période, on considère les tweets EFFECTIVEMENT publiés pdt la période\n","\n","        df_by_period[\"rate_increase\"] = tweets_per_period['GrowthRate']\n","        df_by_period[\"nb_tweets\"] = tweets_per_period[\"TweetCount\"]\n","        df_by_period.loc[0,\"rate_increase\"] = 0\n","        df_by_period[\"labels\"] = labels[\"EventType\"]\n","\n","        df['Tweet'] = df['Tweet'].apply(lambda x: x.split(\": \", 1)[1] if x.startswith(\"RT\") and \": \" in x else x) #supprimer le \"RT ... : \"\n","        df['tweet_occ'] = df['Tweet'].map(df['Tweet'].value_counts())\n","        df = df.drop_duplicates(subset=['Tweet']).reset_index(drop=True)\n","\n","        tweets_RT_per_period = df.groupby('PeriodID')[\"tweet_occ\"].sum().reset_index()\n","\n","        df_by_period[\"nb_tweets_RT\"] = tweets_RT_per_period[\"tweet_occ\"]\n","\n","        df['Tweet'] = df['Tweet'].apply(preprocess_text)\n","\n","        # Apply preprocessing to each tweet and obtain vectors\n","        vector_size = 200  # Adjust based on the chosen GloVe model\n","        tweet_vectors = np.vstack([get_avg_embedding(tweet, embeddings_model, vector_size) for tweet in df['Tweet']])\n","        tweet_df = pd.DataFrame(tweet_vectors)\n","\n","        # Attach the vectors into the original dataframe\n","        period_features = pd.concat([df, tweet_df], axis=1)\n","        # Drop the columns that are not useful anymore\n","        period_features = period_features.drop(columns=['Timestamp', 'Tweet'])\n","        # Group the tweets into their corresponding periods. This way we generate an average embedding vector for each period\n","        period_features = period_features.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n","\n","        drop =period_features.drop(['MatchID', 'PeriodID', 'ID', 'EventType','tweet_occ'], axis=1)\n","\n","        df_preproc = pd.concat([df_by_period, drop], axis=1)\n","\n","        dataframes.append(df_preproc)\n","\n","df_train = pd.concat(dataframes)\n","df_train.to_csv(path_training_dest, index=False)"]},{"cell_type":"markdown","source":["### Preprocessing of the test data"],"metadata":{"id":"Tzr5-9XB4pVh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vqY-D2mn3bBf"},"outputs":[],"source":["dataframes_test = []\n","\n","for fichier in os.listdir(test_path):\n","    if fichier.endswith('.csv'):\n","        chemin_complet = os.path.join(test_path, fichier)\n","        df = pd.read_csv(chemin_complet)\n","        period_max = np.max(df[\"PeriodID\"].tolist())\n","        df_by_period = pd.DataFrame()\n","        df_by_period[\"PeriodID\"] = np.arange(0,period_max+1,1)\n","\n","        #pour le taux d'accroissement, on ne considère que les tweets EFFECTIVEMENT publiés pdt la période\n","        tweets_per_period = df.groupby('PeriodID').size().reset_index(name='TweetCount')\n","        tweets_per_period['GrowthRate'] = tweets_per_period['TweetCount'].pct_change() * 100\n","\n","        #pour le nb de tweets par période, on considère les tweets EFFECTIVEMENT publiés pdt la période\n","\n","        df_by_period[\"rate_increase\"] = tweets_per_period['GrowthRate']\n","        df_by_period[\"nb_tweets\"] = tweets_per_period[\"TweetCount\"]\n","        df_by_period.loc[0,\"rate_increase\"] = 0\n","\n","        df['Tweet'] = df['Tweet'].apply(lambda x: x.split(\": \", 1)[1] if x.startswith(\"RT\") and \": \" in x else x) #supprimer le \"RT ... : \"\n","        df['tweet_occ'] = df['Tweet'].map(df['Tweet'].value_counts())\n","        df = df.drop_duplicates(subset=['Tweet']).reset_index(drop=True)\n","\n","        tweets_RT_per_period = df.groupby('PeriodID')[\"tweet_occ\"].sum().reset_index()\n","\n","        df_by_period[\"nb_tweets_RT\"] = tweets_RT_per_period[\"tweet_occ\"]\n","\n","        df['Tweet'] = df['Tweet'].apply(preprocess_text)\n","\n","        # Apply preprocessing to each tweet and obtain vectors\n","        vector_size = 200  # Adjust based on the chosen GloVe model\n","        tweet_vectors = np.vstack([get_avg_embedding(tweet, embeddings_model, vector_size) for tweet in df['Tweet']])\n","        tweet_df = pd.DataFrame(tweet_vectors)\n","\n","        # Attach the vectors into the original dataframe\n","        period_features = pd.concat([df, tweet_df], axis=1)\n","        # Drop the columns that are not useful anymore\n","        period_features = period_features.drop(columns=['Timestamp', 'Tweet'])\n","        # Group the tweets into their corresponding periods. This way we generate an average embedding vector for each period\n","        period_features = period_features.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n","\n","        drop =period_features.drop(['MatchID', 'PeriodID', 'tweet_occ'], axis=1)\n","\n","        df_preproc = pd.concat([df_by_period, drop], axis=1)\n","\n","        dataframes_test.append(df_preproc)\n","\n","df_test = pd.concat(dataframes_test)\n","df_test.to_csv(path_test_dest, index=False)"]}],"metadata":{"kernelspec":{"display_name":"vic_3A","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}